# -*- coding: utf-8 -*-
"""Lepto_CCHAIN_Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14J-LkYZT6T3fqbvbFjwydQTiOZB_BoJQ

#Initialization
"""

!pip install pandas scikit-learn

!pip install shap

!pip install streamlit

# general libraries
import re
import time
import json
import pickle
import warnings
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm
from collections import Counter
warnings.filterwarnings("ignore")

# visualizations
import seaborn as sns
from termcolor import colored
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# modelling
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import (train_test_split, GridSearchCV,
                                     StratifiedKFold)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (RandomForestClassifier,
                              GradientBoostingClassifier)
from sklearn.model_selection import cross_validate
from sklearn.metrics import (ConfusionMatrixDisplay, precision_score, recall_score)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score

#linear
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import time
import matplotlib.pyplot as plt
from tabulate import tabulate

# imbalanced techniques
from imblearn.pipeline import Pipeline, make_pipeline
from imblearn.over_sampling import (SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE)
from imblearn.under_sampling import (TomekLinks, NearMiss, AllKNN,
                                     EditedNearestNeighbours,
                                     RepeatedEditedNearestNeighbours)
from imblearn.combine import SMOTETomek, SMOTEENN

# interpretability
import shap

# model deployment
from flask import Flask
import streamlit as st

# mount gdrive
from google.colab import drive
drive.mount('/content/drive')

lepto_df = pd.read_csv('/content/drive/MyDrive/Leptospirosis CCHAIN/lepto_dfclean.csv')
lepto_df.head()

"""# Linear Regression

Sample Interpretation:

Interpretation for Linear Regression in Dagupan:

* RÂ² value of 0.15 indicates that 14.79% of the variance in the target variable can be explained by the features.
* MSE of 3.50 represents the average squared difference between the observed actual outcomes and the outcomes predicted by the model.
* MAE of 0.99 represents the average absolute difference between the observed actual outcomes and the outcomes predicted by the model.
"""

# Dictionary to store the results per city
city_results = {}

# Models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'ElasticNet Regression': ElasticNet()
}

# List of cities (unique values from adm3_en)
cities = lepto_df['adm3_en'].unique()

# Loop over each city
for city in cities:
    # Filter data for the current city
    city_data = lepto_df[lepto_df['adm3_en'] == city].copy()

    # Drop the 'date' and 'adm3_en' columns
    city_data.drop(columns=['date', 'adm3_en'], inplace=True)

    # Split the data into features (X) and target (y)
    X = city_data.drop(columns=['case_total'])
    y = city_data['case_total']

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)

    # Standardize the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # List to store the results for the current city
    city_model_results = []

    # Loop over each model
    for model_name, model in models.items():
        # Baseline Model Training
        model.fit(X_train_scaled, y_train)

        # Make predictions
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)

        # Calculate metrics
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        train_mse = mean_squared_error(y_train, y_train_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        train_rmse = np.sqrt(train_mse)
        test_rmse = np.sqrt(test_mse)
        train_mae = mean_absolute_error(y_train, y_train_pred)
        test_mae = mean_absolute_error(y_test, y_test_pred)

        # Store the results in the list for the current city
        city_model_results.append({
            'Model': model_name,
            'Train R^2': train_r2,
            'Test R^2': test_r2,
            'Train RMSE': train_rmse,
            'Test RMSE': test_rmse,
            'Train MSE': train_mse,
            'Test MSE': test_mse,
            'Train MAE': train_mae,
            'Test MAE': test_mae
        })

    # Convert the results list for the current city into a DataFrame
    city_results_df = pd.DataFrame(city_model_results)

    # Store the DataFrame in the dictionary with the city name as the key
    city_results[city] = city_results_df

# Sort the cities by the highest Test R^2 (Val R^2)
sorted_cities = sorted(city_results.items(), key=lambda x: x[1]['Test R^2'].max(), reverse=True)

# Display the sorted results with the specified styling
for city, results_df in sorted_cities:
    print(f"\nMetrics for {city}:")

    # Style the DataFrame
    styled_df = results_df.style.format({
        'Train R^2': '{:.2f}',
        'Test R^2': '{:.2f}',
        'Train RMSE': '{:.2f}',
        'Test RMSE': '{:.2f}',
        'Train MSE': '{:.2f}',
        'Test MSE': '{:.2f}',
        'Train MAE': '{:.2f}',
        'Test MAE': '{:.2f}',
    }).set_table_styles([
        {'selector': 'th', 'props': [('text-align', 'center')]},
        {'selector': 'td', 'props': [('text-align', 'center')]}
    ]).set_properties(**{'border': '1px solid black'})

    display(styled_df)

"""# Binary Classification"""

lepto_df = pd.read_csv('/content/drive/MyDrive/Leptospirosis CCHAIN/lepto_dfclean.csv')
lepto_df.head()

lepto_df['case_total'] = lepto_df['case_total'].apply(lambda x: x > 0)
lepto_df = lepto_df
# This will convert the 'case_total' to True if the value is greater than 0, otherwise False
lepto_df.head()

# Count the occurrences of cases with and without cases
case_counts = lepto_df['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()

# Calculate percentages
total_counts = case_counts.sum()
percentages = (case_counts / total_counts * 100).round(2)

# Print the percentages
print(f"Percentage of With Case: {percentages['With Case']}%")
print(f"Percentage of Without Case: {percentages['Without Case']}%")

# Plotting the bar graph
plt.figure(figsize=(10, 6))

# Define bar positions and width
bar_positions = np.arange(len(case_counts))
bar_width = 0.7  # Reduced width for closer bars

# Plot bars
plt.bar(bar_positions, case_counts.values, color='#19535B', width=bar_width)

# Customize x-axis
plt.xticks(bar_positions, case_counts.index)

# Removing gridlines
plt.grid(False)

# Show plot
plt.show()

total = pd.read_csv('/content/drive/MyDrive/Leptospirosis CCHAIN/total_cases_per_city.csv')
print(total)

# Sort the DataFrame by 'case_total' in descending order
total_sorted = total.sort_values(by='case_total', ascending=False)

# Display the cleaned DataFrame
print("\nCleaned and Sorted Data (Highest to Lowest):")
print(total_sorted)

# Sort the DataFrame by 'case_total' in descending order
total_sorted = total.sort_values(by='case_total', ascending=False)

# Define colors: Top 3 highlighted, rest in gray
colors = ['#19535B' if i < 3 else 'gray' for i in range(len(total_sorted))]

# Create a horizontal bar plot
plt.figure(figsize=(12, 8))
bars = plt.barh(total_sorted['adm3_en'], total_sorted['case_total'], color=colors)
plt.gca().invert_yaxis()  # To have the highest values at the top

# Add value labels beside the bars
for bar in bars:
    width = bar.get_width()
    plt.text(width + (width * 0.01), bar.get_y() + bar.get_height()/2, f'{int(width)}',
             va='center', ha='left', fontsize=12, color='black')

# Customize font sizes
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)

# Adjust the plot margins to create space around the plot area
plt.margins(x=0.07, y=0.05)  # Adjust margins as needed

# Show the plot
plt.tight_layout()
plt.show()

"""## Baselining"""

# Dictionary to store the results per city
city_results = {}

# Models to evaluate
models = {
    'Logistic Regression': LogisticRegression(random_state=1337),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=1337),
    'Random Forest': RandomForestClassifier(random_state=1337),
    'Gradient Boosting': GradientBoostingClassifier(random_state=1337)
}

# Sort the cities by case_total from highest to lowest using total_sorted DataFrame
sorted_cities = total_sorted['adm3_en']

# Loop over each city in the sorted order
for city in sorted_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Filter data for the current city
    city_data = lepto_df[lepto_df['adm3_en'] == city].copy()

    # Count the occurrences of cases with and without cases for the current city
    case_counts = city_data['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    # Drop the 'date', 'adm3_en', and 'case_total' columns
    city_data.drop(columns=['date', 'adm3_en', 'case_total'], inplace=True)

    # Split the data into features (X) and target (y)
    X = city_data
    y = lepto_df.loc[lepto_df['adm3_en'] == city, 'case_total']

    # Stratified Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        random_state=11, test_size=0.25,
                                                        stratify=y)

    # Standardize the features using MinMaxScaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Stratified K-Fold Cross-Validation
    skf = StratifiedKFold(n_splits=5)

    # Loop over each model
    city_model_results = []
    for model_name, model in models.items():
        # Lists to store the scores for cross-validation
        train_accuracies = []
        test_accuracies = []
        train_precisions = []
        test_precisions = []
        train_recalls = []
        test_recalls = []
        train_f1_scores = []
        test_f1_scores = []

        for train_index, val_index in skf.split(X_train_scaled, y_train):
            X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]
            y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]

            # Fit the model
            start = time.time()
            model.fit(X_train_cv, y_train_cv)
            end = time.time()
            runtime = end - start

            # Predictions
            y_train_pred = model.predict(X_train_cv)
            y_val_pred = model.predict(X_val_cv)

            # Calculate metrics
            train_accuracies.append(accuracy_score(y_train_cv, y_train_pred))
            test_accuracies.append(accuracy_score(y_val_cv, y_val_pred))

            train_precisions.append(precision_score(y_train_cv, y_train_pred))
            test_precisions.append(precision_score(y_val_cv, y_val_pred))

            train_recalls.append(recall_score(y_train_cv, y_train_pred))
            test_recalls.append(recall_score(y_val_cv, y_val_pred))

            train_f1_scores.append(f1_score(y_train_cv, y_train_pred))
            test_f1_scores.append(f1_score(y_val_cv, y_val_pred))

        # Store the results in the list for the current city
        city_model_results.append({
            'Model': model_name,
            'Train Accuracy': np.mean(train_accuracies),
            'Test Accuracy': np.mean(test_accuracies),
            'Train Precision': np.mean(train_precisions),
            'Test Precision': np.mean(test_precisions),
            'Train Recall': np.mean(train_recalls),
            'Test Recall': np.mean(test_recalls),
            'Train F1 Score': np.mean(train_f1_scores),
            'Test F1 Score': np.mean(test_f1_scores),
            'Runtime (s)': np.mean(runtime)  # Calculate average runtime if needed
        })

    # Convert the results to a DataFrame and store it in the dictionary with the city name as the key
    city_results[city] = pd.DataFrame(city_model_results)

# Display the results with the specified styling
for city in sorted_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Get the results DataFrame for the current city
    results_df = city_results[city]

    # Retrieve the counts for the current city
    case_counts = lepto_df[lepto_df['adm3_en'] == city]['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    print(f"\nMetrics for {city} (Total Number of Cases: {case_total}, Weeks With Case: {with_case_count}, Weeks Without Case: {without_case_count}):")

    # Style the DataFrame
    styled_df = results_df.style.format({
        'Train Accuracy': '{:.2f}',
        'Test Accuracy': '{:.2f}',
        'Train Precision': '{:.2f}',
        'Test Precision': '{:.2f}',
        'Train Recall': '{:.2f}',
        'Test Recall': '{:.2f}',
        'Train F1 Score': '{:.2f}',
        'Test F1 Score': '{:.2f}',
        'Runtime (s)': '{:.2f}'
    }).set_table_styles([
        {'selector': 'th', 'props': [('text-align', 'center')]},
        {'selector': 'td', 'props': [('text-align', 'center')]}
    ]).set_properties(**{'border': '1px solid black'})

    display(styled_df)

# Select the top 5 cities with the most number of cases
top_5_cities = total_sorted.nlargest(5, 'case_total')['adm3_en']

# Loop over each city in the top 5 cities
for city in top_5_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Filter data for the current city
    city_data = lepto_df[lepto_df['adm3_en'] == city].copy()

    # Count the occurrences of cases with and without cases for the current city
    case_counts = city_data['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    # Drop the 'date', 'adm3_en', and 'case_total' columns
    city_data.drop(columns=['date', 'adm3_en', 'case_total'], inplace=True)

    # Split the data into features (X) and target (y)
    X = city_data
    y = lepto_df.loc[lepto_df['adm3_en'] == city, 'case_total']

    # Stratified Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        random_state=11, test_size=0.25,
                                                        stratify=y)

    # Standardize the features using MinMaxScaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Stratified K-Fold Cross-Validation
    skf = StratifiedKFold(n_splits=5)

    # Loop over each model
    city_model_results = []
    for model_name, model in models.items():
        # Lists to store the scores for cross-validation
        train_accuracies = []
        test_accuracies = []
        train_precisions = []
        test_precisions = []
        train_recalls = []
        test_recalls = []
        train_f1_scores = []
        test_f1_scores = []

        for train_index, val_index in skf.split(X_train_scaled, y_train):
            X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]
            y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]

            # Fit the model
            start = time.time()
            model.fit(X_train_cv, y_train_cv)
            end = time.time()
            runtime = end - start

            # Predictions
            y_train_pred = model.predict(X_train_cv)
            y_val_pred = model.predict(X_val_cv)

            # Calculate metrics
            train_accuracies.append(accuracy_score(y_train_cv, y_train_pred))
            test_accuracies.append(accuracy_score(y_val_cv, y_val_pred))

            train_precisions.append(precision_score(y_train_cv, y_train_pred))
            test_precisions.append(precision_score(y_val_cv, y_val_pred))

            train_recalls.append(recall_score(y_train_cv, y_train_pred))
            test_recalls.append(recall_score(y_val_cv, y_val_pred))

            train_f1_scores.append(f1_score(y_train_cv, y_train_pred))
            test_f1_scores.append(f1_score(y_val_cv, y_val_pred))

        # Store the results in the list for the current city
        city_model_results.append({
            'Model': model_name,
            'Train Accuracy': np.mean(train_accuracies),
            'Test Accuracy': np.mean(test_accuracies),
            'Train Precision': np.mean(train_precisions),
            'Test Precision': np.mean(test_precisions),
            'Train Recall': np.mean(train_recalls),
            'Test Recall': np.mean(test_recalls),
            'Train F1 Score': np.mean(train_f1_scores),
            'Test F1 Score': np.mean(test_f1_scores),
            'Runtime (s)': np.mean(runtime)  # Calculate average runtime if needed
        })

    # Convert the results to a DataFrame and store it in the dictionary with the city name as the key
    city_results[city] = pd.DataFrame(city_model_results)

# Display the results with the specified styling and highlight the best Test Accuracy
for city in top_5_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Get the results DataFrame for the current city
    results_df = city_results[city]

    # Retrieve the counts for the current city
    case_counts = lepto_df[lepto_df['adm3_en'] == city]['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    print(f"\nMetrics for {city} (Total Number of Cases: {case_total}, Weeks With Case: {with_case_count}, Weeks Without Case: {without_case_count}):")

    # Highlight the best Test Accuracy
    best_test_accuracy = results_df['Test Accuracy'].max()

    # Style the DataFrame
    styled_df = results_df.style.format({
        'Train Accuracy': '{:.2f}',
        'Test Accuracy': '{:.2f}',
        'Train Precision': '{:.2f}',
        'Test Precision': '{:.2f}',
        'Train Recall': '{:.2f}',
        'Test Recall': '{:.2f}',
        'Train F1 Score': '{:.2f}',
        'Test F1 Score': '{:.2f}',
        'Runtime (s)': '{:.2f}'
    }).applymap(lambda v: 'background-color: #19535B; color: white;' if v == best_test_accuracy else '',
               subset=['Test Accuracy']
    ).set_table_styles([
        {'selector': 'th', 'props': [('text-align', 'center')]},
        {'selector': 'td', 'props': [('text-align', 'center')]}
    ]).set_properties(**{'border': '1px solid black'})

    display(styled_df)

"""**Highlight the model with the highest F1 score or the one with the best balance between precision and recall**"""

# Select the top 5 cities with the most number of cases
top_5_cities = total_sorted.nlargest(5, 'case_total')['adm3_en']

# Loop over each city in the top 5 cities
for city in top_5_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Filter data for the current city
    city_data = lepto_df[lepto_df['adm3_en'] == city].copy()

    # Count the occurrences of cases with and without cases for the current city
    case_counts = city_data['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    # Drop the 'date', 'adm3_en', and 'case_total' columns
    city_data.drop(columns=['date', 'adm3_en', 'case_total'], inplace=True)

    # Split the data into features (X) and target (y)
    X = city_data
    y = lepto_df.loc[lepto_df['adm3_en'] == city, 'case_total']

    # Stratified Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        random_state=11, test_size=0.25,
                                                        stratify=y)

    # Standardize the features using MinMaxScaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Stratified K-Fold Cross-Validation
    skf = StratifiedKFold(n_splits=5)

    # Loop over each model
    city_model_results = []
    for model_name, model in models.items():
        # Lists to store the scores for cross-validation
        train_accuracies = []
        test_accuracies = []
        train_precisions = []
        test_precisions = []
        train_recalls = []
        test_recalls = []
        train_f1_scores = []
        test_f1_scores = []

        for train_index, val_index in skf.split(X_train_scaled, y_train):
            X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]
            y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]

            # Fit the model
            start = time.time()
            model.fit(X_train_cv, y_train_cv)
            end = time.time()
            runtime = end - start

            # Predictions
            y_train_pred = model.predict(X_train_cv)
            y_val_pred = model.predict(X_val_cv)

            # Calculate metrics
            train_accuracies.append(accuracy_score(y_train_cv, y_train_pred))
            test_accuracies.append(accuracy_score(y_val_cv, y_val_pred))

            train_precisions.append(precision_score(y_train_cv, y_train_pred))
            test_precisions.append(precision_score(y_val_cv, y_val_pred))

            train_recalls.append(recall_score(y_train_cv, y_train_pred))
            test_recalls.append(recall_score(y_val_cv, y_val_pred))

            train_f1_scores.append(f1_score(y_train_cv, y_train_pred))
            test_f1_scores.append(f1_score(y_val_cv, y_val_pred))

        # Store the results in the list for the current city
        city_model_results.append({
            'Model': model_name,
            'Train Accuracy': np.mean(train_accuracies),
            'Test Accuracy': np.mean(test_accuracies),
            'Train Precision': np.mean(train_precisions),
            'Test Precision': np.mean(test_precisions),
            'Train Recall': np.mean(train_recalls),
            'Test Recall': np.mean(test_recalls),
            'Train F1 Score': np.mean(train_f1_scores),
            'Test F1 Score': np.mean(test_f1_scores),
            'Runtime (s)': np.mean(runtime)  # Calculate average runtime if needed
        })

    # Convert the results to a DataFrame and store it in the dictionary with the city name as the key
    city_results[city] = pd.DataFrame(city_model_results)

# Display the results with the specified styling and highlight the best model based on Test F1 Score
for city in top_5_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Get the results DataFrame for the current city
    results_df = city_results[city]

    # Retrieve the counts for the current city
    case_counts = lepto_df[lepto_df['adm3_en'] == city]['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    print(f"\nMetrics for {city} (Total Number of Cases: {case_total}, Weeks With Case: {with_case_count}, Weeks Without Case: {without_case_count}):")

    # Find the best model based on the highest Test F1 Score
    best_f1_score = results_df['Test F1 Score'].max()
    best_models = results_df[results_df['Test F1 Score'] == best_f1_score]

    if len(best_models) > 1:
        # If there's a tie, break it by choosing the one with the highest Test Accuracy
        best_model = best_models.loc[best_models['Test Accuracy'].idxmax()]
    else:
        best_model = best_models.iloc[0]

    # Highlight the best model
    styled_df = results_df.style.format({
        'Train Accuracy': '{:.2f}',
        'Test Accuracy': '{:.2f}',
        'Train Precision': '{:.2f}',
        'Test Precision': '{:.2f}',
        'Train Recall': '{:.2f}',
        'Test Recall': '{:.2f}',
        'Train F1 Score': '{:.2f}',
        'Test F1 Score': '{:.2f}',
        'Runtime (s)': '{:.2f}'
    }).applymap(lambda v: 'background-color: #19535B; color: white;' if v == best_f1_score else '',
               subset=['Test F1 Score']
    ).set_table_styles([
        {'selector': 'th', 'props': [('text-align', 'center')]},
        {'selector': 'td', 'props': [('text-align', 'center')]}
    ]).set_properties(**{'border': '1px solid black'})

    display(styled_df)

# Initialize a list to store the summary of the best models per city
summary_results = []

# Loop through the results for each city and summarize the best model
for city in top_5_cities:
    # Retrieve the case_total for the current city
    case_total = total_sorted.loc[total_sorted['adm3_en'] == city, 'case_total'].values[0]

    # Get the results DataFrame for the current city
    results_df = city_results[city]

    # Retrieve the counts for the current city
    case_counts = lepto_df[lepto_df['adm3_en'] == city]['case_total'].apply(lambda x: 'With Case' if x > 0 else 'Without Case').value_counts()
    with_case_count = case_counts.get('With Case', 0)
    without_case_count = case_counts.get('Without Case', 0)

    # Find the best model based on the highest Test F1 Score
    best_f1_score = results_df['Test F1 Score'].max()
    best_models = results_df[results_df['Test F1 Score'] == best_f1_score]

    if len(best_models) > 1:
        # If there's a tie, break it by choosing the one with the highest Test Accuracy
        best_model = best_models.loc[best_models['Test Accuracy'].idxmax()]
    else:
        best_model = best_models.iloc[0]

    # Store the best model summary for the city
    summary_results.append({
        'City': city,
        'Total Number of Cases': case_total,
        'Weeks with Case': with_case_count,
        'Weeks without Case': without_case_count,
        'Best Model': best_model['Model'],
        'Train Accuracy': best_model['Train Accuracy'],
        'Test Accuracy': best_model['Test Accuracy'],
        'Train Precision': best_model['Train Precision'],
        'Test Precision': best_model['Test Precision'],
        'Train Recall': best_model['Train Recall'],
        'Test Recall': best_model['Test Recall'],
        'Train F1 Score': best_model['Train F1 Score'],
        'Test F1 Score': best_model['Test F1 Score'],
        'Runtime (s)': best_model['Runtime (s)']
    })

# Convert the summary results into a DataFrame
summary_df = pd.DataFrame(summary_results)

# Style the summary DataFrame
styled_summary_df = summary_df.style.format({
    'Total Number of Cases': '{:.0f}',
    'Weeks with Case': '{:.0f}',
    'Weeks without Case': '{:.0f}',
    'Train Accuracy': '{:.2f}',
    'Test Accuracy': '{:.2f}',
    'Train Precision': '{:.2f}',
    'Test Precision': '{:.2f}',
    'Train Recall': '{:.2f}',
    'Test Recall': '{:.2f}',
    'Train F1 Score': '{:.2f}',
    'Test F1 Score': '{:.2f}',
    'Runtime (s)': '{:.2f}'
}).set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},
    {'selector': 'td', 'props': [('text-align', 'center')]}
]).set_properties(**{'border': '1px solid black'})

# Display the styled summary DataFrame
display(styled_summary_df)

"""## Hypertuning"""

from sklearn.model_selection import GridSearchCV

# Hyperparameter grids for each model
param_grids = {
    'Logistic Regression': {
        'C': [0.01, 0.1, 1, 10, 100],
        'solver': ['liblinear', 'saga']
    },
    'KNN': {
        'n_neighbors': [3, 5, 7, 9, 11],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    },
    'Decision Tree': {
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'criterion': ['gini', 'entropy']
    },
    'Random Forest': {
        'n_estimators': [100, 200, 500],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'criterion': ['gini', 'entropy']
    },
    'Gradient Boosting': {
        'n_estimators': [100, 200, 500],
        'learning_rate': [0.01, 0.1, 0.2, 0.3],
        'max_depth': [3, 5, 7],
        'subsample': [0.7, 0.8, 0.9, 1.0]
    }
}

# Dictionary to store the hypertuned results
hypertuned_results = []

# Loop over each city and hypertune the best model
for index, row in summary_df.iterrows():
    city = row['City']
    best_model_name = row['Best Model']

    # Filter data for the current city
    city_data = lepto_df[lepto_df['adm3_en'] == city].copy()

    # Drop the 'date', 'adm3_en', and 'case_total' columns
    X = city_data.drop(columns=['date', 'adm3_en', 'case_total'])
    y = city_data['case_total']

    # Stratified Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11, test_size=0.25, stratify=y)

    # Standardize the features using MinMaxScaler
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Get the best model instance
    best_model = models[best_model_name]

    # Define the GridSearchCV
    grid_search = GridSearchCV(
        estimator=best_model,
        param_grid=param_grids[best_model_name],
        scoring='f1',
        cv=5,
        n_jobs=-1,
        verbose=1
    )

    # Fit GridSearchCV
    start = time.time()
    grid_search.fit(X_train_scaled, y_train)
    end = time.time()
    runtime = end - start

    # Get the best estimator from the grid search
    best_estimator = grid_search.best_estimator_

    # Evaluate on the test set
    y_train_pred = best_estimator.predict(X_train_scaled)
    y_test_pred = best_estimator.predict(X_test_scaled)

    # Calculate metrics
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    train_precision = precision_score(y_train, y_train_pred)
    test_precision = precision_score(y_test, y_test_pred)
    train_recall = recall_score(y_train, y_train_pred)
    test_recall = recall_score(y_test, y_test_pred)
    train_f1 = f1_score(y_train, y_train_pred)
    test_f1 = f1_score(y_test, y_test_pred)

    # Append the results to the hypertuned_results list
    hypertuned_results.append({
        'City': city,
        'Total Number of Cases': row['Total Number of Cases'],
        'Weeks with Case': row['Weeks with Case'],
        'Weeks without Case': row['Weeks without Case'],
        'Best Model': best_model_name,
        'Train Accuracy': train_accuracy,
        'Test Accuracy': test_accuracy,
        'Train Precision': train_precision,
        'Test Precision': test_precision,
        'Train Recall': train_recall,
        'Test Recall': test_recall,
        'Train F1 Score': train_f1,
        'Test F1 Score': test_f1,
        'Runtime (s)': runtime,
        'Best Parameters': grid_search.best_params_
    })

# Convert the results to a DataFrame
hypertuned_summary_df = pd.DataFrame(hypertuned_results)

# Style the DataFrame as before
styled_hypertuned_summary_df = hypertuned_summary_df.style.format({
    'Train Accuracy': '{:.2f}',
    'Test Accuracy': '{:.2f}',
    'Train Precision': '{:.2f}',
    'Test Precision': '{:.2f}',
    'Train Recall': '{:.2f}',
    'Test Recall': '{:.2f}',
    'Train F1 Score': '{:.2f}',
    'Test F1 Score': '{:.2f}',
    'Runtime (s)': '{:.2f}',
}).set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},
    {'selector': 'td', 'props': [('text-align', 'center')]}
]).set_properties(**{'border': '1px solid black'})

# Display the styled DataFrame
display(styled_hypertuned_summary_df)

"""## SHAP

### Iloilo
"""

import shap
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Filter data for Iloilo
iloilo_data = lepto_df[lepto_df['adm3_en'] == 'Iloilo'].copy()

# Check if there's data available for Iloilo
if iloilo_data.shape[0] == 0:
    raise ValueError("No data available for Iloilo. Please check the filtering criteria.")

# Drop the 'date', 'adm3_en', and 'case_total' columns to get features
X = iloilo_data.drop(columns=['date', 'adm3_en', 'case_total'])
y = iloilo_data['case_total']

# Stratified Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11, test_size=0.25, stratify=y)

# Standardize the features using MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the best Logistic Regression model
best_model = LogisticRegression(C=10, solver='liblinear', random_state=11)
best_model.fit(X_train_scaled, y_train)

# Use SHAP's LinearExplainer to generate SHAP values
explainer = shap.LinearExplainer(best_model, X_train_scaled)
shap_values = explainer.shap_values(X_test_scaled)

# Calculate feature importance from SHAP values
mean_shap_values = np.mean(np.abs(shap_values), axis=0)
feature_importance = pd.DataFrame(list(zip(X.columns, mean_shap_values, shap_values.mean(axis=0))),
                                  columns=['Feature', 'Mean Absolute SHAP Value', 'Mean SHAP Value'])

# Sort the features from highest positive to lowest negative impact
feature_importance.sort_values(by='Mean SHAP Value', ascending=False, inplace=True)

# Style the feature importance DataFrame
styled_feature_importance = feature_importance.style.format({
    'Mean Absolute SHAP Value': '{:.4f}',
    'Mean SHAP Value': '{:.4f}'
}).set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},
    {'selector': 'td', 'props': [('text-align', 'center')]}
]).set_properties(**{'border': '1px solid black'})

# Display the styled DataFrame
display(styled_feature_importance)

# Display SHAP summary plot
shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, plot_type="bar")

# Automated Interpretation
interpretation = """
Key Interpretations:
- The SHAP summary plot ranks features by their average absolute impact (Mean Absolute SHAP Value) on the model's predictions, regardless of direction.
- Features like '{}' with a high Mean Absolute SHAP Value have a significant impact on model predictions, while '{}', though lower in the ranking, still have a non-negligible effect.
- The Mean SHAP Value column shows the direction of each feature's influence on predictions: '{}' has a strong positive impact, while '{}' has a noticeable negative impact.
""".format(feature_importance.iloc[0]['Feature'],
           feature_importance.iloc[-1]['Feature'],
           feature_importance[feature_importance['Mean SHAP Value'] > 0].iloc[0]['Feature'],
           feature_importance[feature_importance['Mean SHAP Value'] < 0].iloc[0]['Feature'])

# Print interpretation notes
print(interpretation)

"""### CdO"""

import shap
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Convert 'case_total' to True if greater than 0, otherwise False
lepto_df['case_total'] = lepto_df['case_total'].apply(lambda x: x > 0)

# Filter data for Cagayan de Oro
cdo_data = lepto_df[lepto_df['adm3_en'] == 'Cagayan de Oro'].copy()

# Check if there's data available for Cagayan de Oro
if cdo_data.shape[0] == 0:
    raise ValueError("No data available for Cagayan de Oro. Please check the filtering criteria.")

# Drop the 'date', 'adm3_en', and 'case_total' columns to get features
X = cdo_data.drop(columns=['date', 'adm3_en', 'case_total'])
y = cdo_data['case_total']

# Stratified Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11, test_size=0.25, stratify=y)

# Standardize the features using MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the best Decision Tree model
best_model = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_leaf=1, min_samples_split=2, random_state=11)
best_model.fit(X_train_scaled, y_train)

# Use SHAP's KernelExplainer
explainer = shap.KernelExplainer(best_model.predict_proba, X_train_scaled)
shap_values = explainer.shap_values(X_test_scaled)

# Debugging the shapes
print("Shape of SHAP Values:", np.array(shap_values).shape)
print("Shape of X_test_scaled:", X_test_scaled.shape)

# Calculate feature importance from SHAP values (summing across classes)
summed_shap_values = np.sum(shap_values, axis=0)
mean_shap_values = np.mean(np.abs(summed_shap_values), axis=0)
feature_importance = pd.DataFrame(list(zip(X.columns, mean_shap_values, summed_shap_values.mean(axis=0))),
                                  columns=['Feature', 'Mean Absolute SHAP Value', 'Mean SHAP Value'])

# Sort features from highest positive to lowest negative
feature_importance = feature_importance.sort_values(by='Mean SHAP Value', ascending=False)

# Style the feature importance DataFrame
styled_feature_importance = feature_importance.style.format({
    'Mean Absolute SHAP Value': '{:.4f}',
    'Mean SHAP Value': '{:.4f}'
}).set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},
    {'selector': 'td', 'props': [('text-align', 'center')]}
]).set_properties(**{'border': '1px solid black'})

# Display the styled DataFrame
display(styled_feature_importance)

# SHAP summary plot
shap.summary_plot(summed_shap_values, X_test_scaled, feature_names=X.columns, plot_type="bar")

"""### Davao

### Dagupan

### Navotas
"""

import shap
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Filter data for Navotas
navotas_data = lepto_df[lepto_df['adm3_en'] == 'Navotas'].copy()

# Check if there's data available for Navotas
if navotas_data.shape[0] == 0:
    raise ValueError("No data available for Navotas. Please check the filtering criteria.")

# Drop the 'date', 'adm3_en', and 'case_total' columns to get features
X = navotas_data.drop(columns=['date', 'adm3_en', 'case_total'])
y = navotas_data['case_total']

# Stratified Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11, test_size=0.25, stratify=y)

# Standardize the features using MinMaxScaler
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the best KNN model
best_model = KNeighborsClassifier(metric='manhattan', n_neighbors=3, weights='distance')
best_model.fit(X_train_scaled, y_train)

# Use SHAP's KernelExplainer
explainer = shap.KernelExplainer(best_model.predict, X_train_scaled)
shap_values = explainer.shap_values(X_test_scaled)

# Calculate feature importance from SHAP values
mean_shap_values = np.mean(np.abs(shap_values), axis=0)
feature_importance = pd.DataFrame(list(zip(X.columns, mean_shap_values, shap_values.mean(axis=0))),
                                  columns=['Feature', 'Mean Absolute SHAP Value', 'Mean SHAP Value'])

# Sort features from highest positive to lowest negative
feature_importance = feature_importance.sort_values(by='Mean SHAP Value', ascending=False)

# Style the feature importance DataFrame
styled_feature_importance = feature_importance.style.format({
    'Mean Absolute SHAP Value': '{:.4f}',
    'Mean SHAP Value': '{:.4f}'
}).set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},
    {'selector': 'td', 'props': [('text-align', 'center')]}
]).set_properties(**{'border': '1px solid black'})

# Display the styled DataFrame
display(styled_feature_importance)

# Display SHAP summary plot
shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, plot_type="bar")

# Automated Interpretation
interpretation = """
Key Interpretations:
- The SHAP summary plot ranks features by their average absolute impact (Mean Absolute SHAP Value) on the model's predictions, regardless of direction.
- Features like '{}' with a high Mean Absolute SHAP Value have a significant impact on model predictions, while '{}', though lower in the ranking, still have a non-negligible effect.
- The Mean SHAP Value column shows the direction of each feature's influence on predictions: '{}' has a strong positive impact, while '{}' has a noticeable negative impact.
""".format(feature_importance.iloc[0]['Feature'],
           feature_importance.iloc[-1]['Feature'],
           feature_importance[feature_importance['Mean SHAP Value'] > 0].iloc[0]['Feature'],
           feature_importance[feature_importance['Mean SHAP Value'] < 0].iloc[0]['Feature'])

# Print interpretation notes
print(interpretation)